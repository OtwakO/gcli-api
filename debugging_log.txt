Starting the Gemini API proxy...
[19:30:34] INFO     Loaded 7 credentials from oauth_creds_*.json files.
           DEBUG    Using proactor: IocpProactor
INFO:     Started server process [27744]
INFO:     Waiting for application startup.
           INFO     Ensured credential storage directory exists at:
                    E:\Docker\Containers\gemini-rotating-proxy\gcli2api\credentials
           INFO     Proxy is running with 7 credential(s).
           INFO     Starting credential warm-up for 7 credential(s)...
INFO:     Application startup complete.
           DEBUG    - Warming up and refreshing expired credential for speedy-baton-468102-s3 (otwakodev@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for winged-poetry-468121-e8 (otwakogame@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for disco-arcana-468121-q0 (otwakomusic@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for stone-forest-djkxm (secqxawx.dev@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for formidable-rune-468121-g9
                    (secqxawx.game@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for geminicli-468121 (secqxawx.music@gmail.com)
           DEBUG    - Warming up and refreshing expired credential for gemini-agents-458715 (verychillpikachu@gmail.com)
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
INFO:     Uvicorn running on http://0.0.0.0:7860 (Press CTRL+C to quit)
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    Starting new HTTPS connection (1): oauth2.googleapis.com:443
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    - Credential for formidable-rune-468121-g9 (secqxawx.game@gmail.com) refreshed successfully during
                    warm-up.
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    - Credential for winged-poetry-468121-e8 (otwakogame@gmail.com) refreshed successfully during
                    warm-up.
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    - Credential for speedy-baton-468102-s3 (otwakodev@gmail.com) refreshed successfully during warm-up.
           DEBUG    - Credential for disco-arcana-468121-q0 (otwakomusic@gmail.com) refreshed successfully during
                    warm-up.
           DEBUG    - Credential for stone-forest-djkxm (secqxawx.dev@gmail.com) refreshed successfully during warm-up.
           DEBUG    https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
           DEBUG    - Credential for geminicli-468121 (secqxawx.music@gmail.com) refreshed successfully during warm-up.
           DEBUG    - Credential for gemini-agents-458715 (verychillpikachu@gmail.com) refreshed successfully during
                    warm-up.
           INFO     Credential warm-up complete. Refreshed: 7, Failed: 0
[19:30:40] DEBUG    --- Incoming Request ---
                    {
                      "method": "POST",
                      "url": "http://localhost:7860/v1/chat/completions",
                      "headers": {
                        "accept": "*/*",
                        "accept-encoding": "gzip, deflate, br",
                        "authorization": "Bearer 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",
                        "content-length": "25886",
                        "content-type": "application/json",
                        "user-agent": "node-fetch",
                        "host": "localhost:7860",
                        "connection": "keep-alive"
                      }
                    }
           INFO     --- Credential Selection ---
                    {
                      "index": 0,
                      "user_email": "otwakodev@gmail.com",
                      "project_id": "speedy-baton-468102-s3",
                      "refresh_token_snippet": "...hY_GI"
                    }
           INFO     Handling OpenAI Chat Completions Request (Streaming)
[19:30:41] INFO     User otwakodev@gmail.com is already onboarded for project speedy-baton-468102-s3.
           DEBUG    --- Upstream Request to Google ---
                    {
                      "url": "https://cloudcode-pa.googleapis.com/v1internal:streamGenerateContent?alt=sse",
                      "headers": {
                        "Authorization": "Bearer
                    ya29.a0AS3H6Nw-vbsdZkVm01qfHEghanLyC35ptlvflcUowM_cHmUvvRBRNBInozoO0j7WwaoByuJM9fUl1fA-MTVoAC60bsrjV
                    JE8iSUTFFf7nm19cRECTLfAiAabAIvC_3cy21Tq4n6d9F_D59NebgDaZN-q1DE20de6dkZLWieSBpWwsS8aCgYKAWgSARASFQHGX
                    2MiTsreMQnVW2KAR47yC0-qoA0182",
                        "Content-Type": "application/json",
                        "User-Agent": "GeminiCLI/0.1.5 (Windows; AMD64)"
                      },
                      "payload": {
                        "model": "gemini-2.5-pro-preview-06-05",
                        "project": "speedy-baton-468102-s3",
                        "request": {
                          "contents": [
                            {
                              "role": "model",
                              "parts": [
                                {
                                  "text": "<REDACTED>"
                                }
                              ]
                            },
                            {
                              "role": "user",
                              "parts": [
                                {
                                  "text": "<REDACTED>"
                                }
                              ]
                            },
                            {
                              "role": "model",
                              "parts": [
                                {
                                  "text": "<REDACTED>"
                                }
                              ]
                            }
                          ],
                          "generationConfig": {
                            "temperature": 1.2,
                            "topP": 0.9,
                            "maxOutputTokens": 8192,
                            "frequencyPenalty": 0.0,
                            "presencePenalty": 0.0
                          },
                          "safetySettings": [
                            {
                              "category": "HARM_CATEGORY_HARASSMENT",
                              "threshold": "BLOCK_NONE"
                            },
                            {
                              "category": "HARM_CATEGORY_HATE_SPEECH",
                              "threshold": "BLOCK_NONE"
                            },
                            {
                              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                              "threshold": "BLOCK_NONE"
                            },
                            {
                              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                              "threshold": "BLOCK_NONE"
                            },
                            {
                              "category": "HARM_CATEGORY_CIVIC_INTEGRITY",
                              "threshold": "BLOCK_NONE"
                            }
                          ],
                          "systemInstruction": {
                            "parts": [
                              {
                                "text": "<REDACTED>"
                              }
                            ]
                          }
                        }
                      }
                    }
[19:31:02] DEBUG    --- Upstream Response from Google ---
                    {
                      "status_code": 200,
                      "headers": {
                        "content-type": "text/event-stream",
                        "content-disposition": "attachment",
                        "vary": "Origin, X-Origin, Referer",
                        "transfer-encoding": "chunked",
                        "date": "Wed, 06 Aug 2025 11:31:17 GMT",
                        "server": "ESF",
                        "x-xss-protection": "0",
                        "x-frame-options": "SAMEORIGIN",
                        "x-content-type-options": "nosniff",
                        "server-timing": "gfet4t7; dur=17610",
                        "alt-svc": "h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000"
                      }
                    }
           ERROR    Generic stream processing error: name 'OpenAIChatCompletionStreamResponse' is not defined
                    ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
                    │ E:\Docker\Containers\gemini-rotating-proxy\gcli2api\src\streaming.py:208 in                      │
                    │ process_stream_for_client                                                                        │
                    │                                                                                                  │
                    │   205 │   │   │   │   if text_content:                                                           │
                    │   206 │   │   │   │   │   full_response_text_for_log.append(text_content)                        │
                    │   207 │   │   │                                                                                  │
                    │ ❱ 208 │   │   │   yield formatter(chunk, **formatter_context)                                    │
                    │   209 │   │                                                                                      │
                    │   210 │   │   if settings.DEBUG and full_response_text_for_log:                                  │
                    │   211 │   │   │   final_log_text = "".join(full_response_text_for_log)                           │
                    │                                                                                                  │
                    │ E:\Docker\Containers\gemini-rotating-proxy\gcli2api\src\streaming.py:172 in format_as_openai_sse │
                    │                                                                                                  │
                    │   169 │   # The transformer returns a dict; we must validate it into a Pydantic model            │
                    │   170 │   # before we can serialize it. This aligns with the non-streaming logic.                │
                    │   171 │   openai_chunk_dict = gemini_stream_chunk_to_openai(chunk, model, response_id)           │
                    │ ❱ 172 │   openai_chunk = OpenAIChatCompletionStreamResponse.model_validate(openai_chunk_dict)    │
                    │   173 │   return f"data: {openai_chunk.model_dump_json(exclude_unset=True)}\n\n"                 │
                    │   174                                                                                            │
                    │   175                                                                                            │
                    ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
                    NameError: name 'OpenAIChatCompletionStreamResponse' is not defined